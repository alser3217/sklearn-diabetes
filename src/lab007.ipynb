{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет\n",
    "Загружаем датасет Diabetes dataset. Он содержит 9 числовых признаков и 1 категориальный, таргетная переменная - количественная мера прогрессирования заболевания через год после начального уровня. Предстоит задача регрессии, надо эту переменную по имеющимся признакам предсказать.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_diabetes()\n",
    "dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['data'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем на train/test выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём torch-датасет, чтобы подготовить данные к обучению нейросети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class DiabetDataset(Dataset):\n",
    "    def __init__(self, tensor_X, tensor_y, device):\n",
    "        self.inputs = tensor_X.to(device)\n",
    "        self.targets = tensor_y.to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "diabet_train = DiabetDataset(torch.Tensor(X_train), torch.Tensor(y_train), device)\n",
    "diabet_test = DiabetDataset(torch.Tensor(X_test), torch.Tensor(y_test), device)\n",
    "\n",
    "diabet_train_dataloader = DataLoader(diabet_train, batch_size=32, shuffle=True)\n",
    "diabet_test_dataloader = DataLoader(diabet_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим простую полносвязную нейронную сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_features=10, out_features=1):\n",
    "        super(SimpleNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features=in_features, out_features=64)\n",
    "        self.linear2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.linear3 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.linear4 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.linear5 = nn.Linear(in_features=64, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        self.dropout(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.relu(self.linear4(x))\n",
    "        x = self.linear5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/400, loss: 353199.294921875\n",
      "epoch: 2/400, loss: 344160.3984375\n",
      "epoch: 3/400, loss: 318558.7431640625\n",
      "epoch: 4/400, loss: 314868.57568359375\n",
      "epoch: 5/400, loss: 372118.962890625\n",
      "epoch: 6/400, loss: 266093.7587890625\n",
      "epoch: 7/400, loss: 202716.0841064453\n",
      "epoch: 8/400, loss: 153778.71728515625\n",
      "epoch: 9/400, loss: 61352.50390625\n",
      "epoch: 10/400, loss: 56601.273986816406\n",
      "epoch: 11/400, loss: 59312.80029296875\n",
      "epoch: 12/400, loss: 50503.615325927734\n",
      "epoch: 13/400, loss: 51144.402587890625\n",
      "epoch: 14/400, loss: 52545.525634765625\n",
      "epoch: 15/400, loss: 47289.923290252686\n",
      "epoch: 16/400, loss: 54683.33154296875\n",
      "epoch: 17/400, loss: 65769.70361328125\n",
      "epoch: 18/400, loss: 45999.40966796875\n",
      "epoch: 19/400, loss: 43187.952887535095\n",
      "epoch: 20/400, loss: 43544.39489746094\n",
      "epoch: 21/400, loss: 43584.09704589844\n",
      "epoch: 22/400, loss: 45364.62939453125\n",
      "epoch: 23/400, loss: 55893.445556640625\n",
      "epoch: 24/400, loss: 46442.40478515625\n",
      "epoch: 25/400, loss: 42285.593017578125\n",
      "epoch: 26/400, loss: 43990.213623046875\n",
      "epoch: 27/400, loss: 41141.02880859375\n",
      "epoch: 28/400, loss: 40203.64855957031\n",
      "epoch: 29/400, loss: 46036.467041015625\n",
      "epoch: 30/400, loss: 40334.986572265625\n",
      "epoch: 31/400, loss: 42194.149169921875\n",
      "epoch: 32/400, loss: 47032.6728515625\n",
      "epoch: 33/400, loss: 38968.452392578125\n",
      "epoch: 34/400, loss: 44967.2822265625\n",
      "epoch: 35/400, loss: 40677.439697265625\n",
      "epoch: 36/400, loss: 36820.230041503906\n",
      "epoch: 37/400, loss: 38382.96435546875\n",
      "epoch: 38/400, loss: 35995.816767692566\n",
      "epoch: 39/400, loss: 37587.099365234375\n",
      "epoch: 40/400, loss: 54608.135986328125\n",
      "epoch: 41/400, loss: 37624.1579284668\n",
      "epoch: 42/400, loss: 37825.53039550781\n",
      "epoch: 43/400, loss: 36728.30334472656\n",
      "epoch: 44/400, loss: 35857.5498046875\n",
      "epoch: 45/400, loss: 34663.420917510986\n",
      "epoch: 46/400, loss: 34573.98344077915\n",
      "epoch: 47/400, loss: 35988.05505371094\n",
      "epoch: 48/400, loss: 44601.2939453125\n",
      "epoch: 49/400, loss: 36508.130126953125\n",
      "epoch: 50/400, loss: 37478.82861328125\n",
      "epoch: 51/400, loss: 36826.378173828125\n",
      "epoch: 52/400, loss: 39976.9091796875\n",
      "epoch: 53/400, loss: 34355.22613143921\n",
      "epoch: 54/400, loss: 49594.41174316406\n",
      "epoch: 55/400, loss: 34231.76938056946\n",
      "epoch: 56/400, loss: 34507.73693847656\n",
      "epoch: 57/400, loss: 35513.8154296875\n",
      "epoch: 58/400, loss: 34027.18818664551\n",
      "epoch: 59/400, loss: 43693.427490234375\n",
      "epoch: 60/400, loss: 35361.291748046875\n",
      "epoch: 61/400, loss: 39172.26184082031\n",
      "epoch: 62/400, loss: 34426.32651722431\n",
      "epoch: 63/400, loss: 44249.43408203125\n",
      "epoch: 64/400, loss: 41114.817626953125\n",
      "epoch: 65/400, loss: 35013.02584838867\n",
      "epoch: 66/400, loss: 35974.18359375\n",
      "epoch: 67/400, loss: 33119.70939254761\n",
      "epoch: 68/400, loss: 38978.07080078125\n",
      "epoch: 69/400, loss: 33733.88655090332\n",
      "epoch: 70/400, loss: 36991.660888671875\n",
      "epoch: 71/400, loss: 38056.79504394531\n",
      "epoch: 72/400, loss: 32728.585708618164\n",
      "epoch: 73/400, loss: 34505.414794921875\n",
      "epoch: 74/400, loss: 34567.624267578125\n",
      "epoch: 75/400, loss: 44329.653564453125\n",
      "epoch: 76/400, loss: 35291.78991699219\n",
      "epoch: 77/400, loss: 40039.577392578125\n",
      "epoch: 78/400, loss: 33411.23309326172\n",
      "epoch: 79/400, loss: 39327.4287109375\n",
      "epoch: 80/400, loss: 32634.225524902344\n",
      "epoch: 81/400, loss: 41386.91259765625\n",
      "epoch: 82/400, loss: 32437.62353515625\n",
      "epoch: 83/400, loss: 33077.763732910156\n",
      "epoch: 84/400, loss: 34331.26379394531\n",
      "epoch: 85/400, loss: 38555.9921875\n",
      "epoch: 86/400, loss: 44403.26123046875\n",
      "epoch: 87/400, loss: 33168.394470214844\n",
      "epoch: 88/400, loss: 36869.74365234375\n",
      "epoch: 89/400, loss: 32905.10775756836\n",
      "epoch: 90/400, loss: 34341.28955078125\n",
      "epoch: 91/400, loss: 34274.14172363281\n",
      "epoch: 92/400, loss: 58273.74645996094\n",
      "epoch: 93/400, loss: 43296.56604003906\n",
      "epoch: 94/400, loss: 36261.700927734375\n",
      "epoch: 95/400, loss: 32737.520629882812\n",
      "epoch: 96/400, loss: 32852.20031738281\n",
      "epoch: 97/400, loss: 34099.981689453125\n",
      "epoch: 98/400, loss: 38671.70153808594\n",
      "epoch: 99/400, loss: 33408.40869140625\n",
      "epoch: 100/400, loss: 32218.493774414062\n",
      "epoch: 101/400, loss: 32879.46337890625\n",
      "epoch: 102/400, loss: 32625.059204101562\n",
      "epoch: 103/400, loss: 32161.357147216797\n",
      "epoch: 104/400, loss: 32139.54151916504\n",
      "epoch: 105/400, loss: 32037.10430908203\n",
      "epoch: 106/400, loss: 34473.30810546875\n",
      "epoch: 107/400, loss: 35999.1083984375\n",
      "epoch: 108/400, loss: 34900.017822265625\n",
      "epoch: 109/400, loss: 32430.06182861328\n",
      "epoch: 110/400, loss: 41911.79296875\n",
      "epoch: 111/400, loss: 32553.67559814453\n",
      "epoch: 112/400, loss: 34141.696044921875\n",
      "epoch: 113/400, loss: 34235.5498046875\n",
      "epoch: 114/400, loss: 34335.67785644531\n",
      "epoch: 115/400, loss: 31891.016998291016\n",
      "epoch: 116/400, loss: 34543.17541503906\n",
      "epoch: 117/400, loss: 31898.921356201172\n",
      "epoch: 118/400, loss: 36228.31848144531\n",
      "epoch: 119/400, loss: 33304.361083984375\n",
      "epoch: 120/400, loss: 31652.854454040527\n",
      "epoch: 121/400, loss: 31633.08447265625\n",
      "epoch: 122/400, loss: 32586.751831054688\n",
      "epoch: 123/400, loss: 34962.14123535156\n",
      "epoch: 124/400, loss: 38464.98095703125\n",
      "epoch: 125/400, loss: 32439.013335227966\n",
      "epoch: 126/400, loss: 31614.388305664062\n",
      "epoch: 127/400, loss: 32098.548461914062\n",
      "epoch: 128/400, loss: 32637.563415527344\n",
      "epoch: 129/400, loss: 34682.182861328125\n",
      "epoch: 130/400, loss: 32300.377197265625\n",
      "epoch: 131/400, loss: 34443.52294921875\n",
      "epoch: 132/400, loss: 32323.653381347656\n",
      "epoch: 133/400, loss: 32035.336059570312\n",
      "epoch: 134/400, loss: 33350.052734375\n",
      "epoch: 135/400, loss: 32164.52325439453\n",
      "epoch: 136/400, loss: 33240.03967285156\n",
      "epoch: 137/400, loss: 35755.61181640625\n",
      "epoch: 138/400, loss: 34219.914794921875\n",
      "epoch: 139/400, loss: 32067.67879486084\n",
      "epoch: 140/400, loss: 32035.708892822266\n",
      "epoch: 141/400, loss: 31669.402053833008\n",
      "epoch: 142/400, loss: 32100.10595703125\n",
      "epoch: 143/400, loss: 33809.63720703125\n",
      "epoch: 144/400, loss: 44497.10986328125\n",
      "epoch: 145/400, loss: 32652.98686981201\n",
      "epoch: 146/400, loss: 32340.252418518066\n",
      "epoch: 147/400, loss: 31538.755447387695\n",
      "epoch: 148/400, loss: 32198.159729003906\n",
      "epoch: 149/400, loss: 32512.120666503906\n",
      "epoch: 150/400, loss: 34001.19445800781\n",
      "epoch: 151/400, loss: 31965.54864501953\n",
      "epoch: 152/400, loss: 34115.59326171875\n",
      "epoch: 153/400, loss: 31536.50933074951\n",
      "epoch: 154/400, loss: 39002.8828125\n",
      "epoch: 155/400, loss: 31777.770309448242\n",
      "epoch: 156/400, loss: 32453.792739868164\n",
      "epoch: 157/400, loss: 32825.62805175781\n",
      "epoch: 158/400, loss: 31979.271514892578\n",
      "epoch: 159/400, loss: 31492.29766845703\n",
      "epoch: 160/400, loss: 38370.26477050781\n",
      "epoch: 161/400, loss: 33179.51110839844\n",
      "epoch: 162/400, loss: 31934.073120117188\n",
      "epoch: 163/400, loss: 32382.63427734375\n",
      "epoch: 164/400, loss: 33453.219482421875\n",
      "epoch: 165/400, loss: 31809.623931884766\n",
      "epoch: 166/400, loss: 32067.53271484375\n",
      "epoch: 167/400, loss: 32553.332397460938\n",
      "epoch: 168/400, loss: 32616.434997558594\n",
      "epoch: 169/400, loss: 40345.42370605469\n",
      "epoch: 170/400, loss: 33235.024963378906\n",
      "epoch: 171/400, loss: 35208.439453125\n",
      "epoch: 172/400, loss: 33299.88818359375\n",
      "epoch: 173/400, loss: 32023.747436523438\n",
      "epoch: 174/400, loss: 31655.601776123047\n",
      "epoch: 175/400, loss: 32975.68249511719\n",
      "epoch: 176/400, loss: 31650.332050323486\n",
      "epoch: 177/400, loss: 32652.690795898438\n",
      "epoch: 178/400, loss: 32098.883544921875\n",
      "epoch: 179/400, loss: 33109.38659667969\n",
      "epoch: 180/400, loss: 40803.026123046875\n",
      "epoch: 181/400, loss: 31714.76547241211\n",
      "epoch: 182/400, loss: 33449.161865234375\n",
      "epoch: 183/400, loss: 31513.295166015625\n",
      "epoch: 184/400, loss: 31297.96138381958\n",
      "epoch: 185/400, loss: 39028.88818359375\n",
      "epoch: 186/400, loss: 31823.042289733887\n",
      "epoch: 187/400, loss: 31513.30637025833\n",
      "epoch: 188/400, loss: 31971.711303710938\n",
      "epoch: 189/400, loss: 31300.65965270996\n",
      "epoch: 190/400, loss: 32435.987060546875\n",
      "epoch: 191/400, loss: 38662.599365234375\n",
      "epoch: 192/400, loss: 54708.84375\n",
      "epoch: 193/400, loss: 33255.279724121094\n",
      "epoch: 194/400, loss: 33667.774169921875\n",
      "epoch: 195/400, loss: 32870.63671875\n",
      "epoch: 196/400, loss: 33125.88586425781\n",
      "epoch: 197/400, loss: 33931.87451171875\n",
      "epoch: 198/400, loss: 41751.619873046875\n",
      "epoch: 199/400, loss: 41276.0927734375\n",
      "epoch: 200/400, loss: 40457.80725097656\n",
      "epoch: 201/400, loss: 32477.65869140625\n",
      "epoch: 202/400, loss: 31807.10620880127\n",
      "epoch: 203/400, loss: 31941.5712890625\n",
      "epoch: 204/400, loss: 40840.54699707031\n",
      "epoch: 205/400, loss: 31260.902312755585\n",
      "epoch: 206/400, loss: 40636.302734375\n",
      "epoch: 207/400, loss: 32825.49444580078\n",
      "epoch: 208/400, loss: 34188.15148925781\n",
      "epoch: 209/400, loss: 34597.693359375\n",
      "epoch: 210/400, loss: 37253.04541015625\n",
      "epoch: 211/400, loss: 35875.65856933594\n",
      "epoch: 212/400, loss: 31456.92755126953\n",
      "epoch: 213/400, loss: 31249.378295898438\n",
      "epoch: 214/400, loss: 32075.86279296875\n",
      "epoch: 215/400, loss: 34297.04528808594\n",
      "epoch: 216/400, loss: 34659.925048828125\n",
      "epoch: 217/400, loss: 33749.0791015625\n",
      "epoch: 218/400, loss: 31925.079040527344\n",
      "epoch: 219/400, loss: 32877.20642089844\n",
      "epoch: 220/400, loss: 31148.613037109375\n",
      "epoch: 221/400, loss: 31001.684211730957\n",
      "epoch: 222/400, loss: 31073.01930999756\n",
      "epoch: 223/400, loss: 31060.311653137207\n",
      "epoch: 224/400, loss: 32162.508911132812\n",
      "epoch: 225/400, loss: 31106.7783203125\n",
      "epoch: 226/400, loss: 32437.914428710938\n",
      "epoch: 227/400, loss: 35477.6083984375\n",
      "epoch: 228/400, loss: 37100.757568359375\n",
      "epoch: 229/400, loss: 30669.081464767456\n",
      "epoch: 230/400, loss: 31682.46728515625\n",
      "epoch: 231/400, loss: 31567.973205566406\n",
      "epoch: 232/400, loss: 34439.21728515625\n",
      "epoch: 233/400, loss: 31313.77061021328\n",
      "epoch: 234/400, loss: 31632.73715209961\n",
      "epoch: 235/400, loss: 33925.668212890625\n",
      "epoch: 236/400, loss: 31935.933959960938\n",
      "epoch: 237/400, loss: 30908.304809570312\n",
      "epoch: 238/400, loss: 34796.64440917969\n",
      "epoch: 239/400, loss: 33625.26525878906\n",
      "epoch: 240/400, loss: 31590.622436523438\n",
      "epoch: 241/400, loss: 32414.440307617188\n",
      "epoch: 242/400, loss: 31828.077270507812\n",
      "epoch: 243/400, loss: 32198.64501953125\n",
      "epoch: 244/400, loss: 31602.787353515625\n",
      "epoch: 245/400, loss: 36125.1005859375\n",
      "epoch: 246/400, loss: 31337.49868774414\n",
      "epoch: 247/400, loss: 31363.057525634766\n",
      "epoch: 248/400, loss: 30984.241357803345\n",
      "epoch: 249/400, loss: 41443.34777832031\n",
      "epoch: 250/400, loss: 35227.25048828125\n",
      "epoch: 251/400, loss: 32300.700805664062\n",
      "epoch: 252/400, loss: 30911.624458551407\n",
      "epoch: 253/400, loss: 40536.20349121094\n",
      "epoch: 254/400, loss: 31792.611602783203\n",
      "epoch: 255/400, loss: 33479.46960449219\n",
      "epoch: 256/400, loss: 37204.03662109375\n",
      "epoch: 257/400, loss: 37695.39904785156\n",
      "epoch: 258/400, loss: 33069.394775390625\n",
      "epoch: 259/400, loss: 30940.769207000732\n",
      "epoch: 260/400, loss: 31161.098419189453\n",
      "epoch: 261/400, loss: 31235.43035888672\n",
      "epoch: 262/400, loss: 30848.77993774414\n",
      "epoch: 263/400, loss: 33073.37841796875\n",
      "epoch: 264/400, loss: 32243.09454345703\n",
      "epoch: 265/400, loss: 34695.93640136719\n",
      "epoch: 266/400, loss: 31440.852661132812\n",
      "epoch: 267/400, loss: 30929.597381591797\n",
      "epoch: 268/400, loss: 30904.235624313354\n",
      "epoch: 269/400, loss: 30719.40233521536\n",
      "epoch: 270/400, loss: 31784.864990234375\n",
      "epoch: 271/400, loss: 30681.21389389038\n",
      "epoch: 272/400, loss: 35156.81237792969\n",
      "epoch: 273/400, loss: 35860.36328125\n",
      "epoch: 274/400, loss: 32960.5439453125\n",
      "epoch: 275/400, loss: 31100.20864868164\n",
      "epoch: 276/400, loss: 31197.187225341797\n",
      "epoch: 277/400, loss: 32836.193603515625\n",
      "epoch: 278/400, loss: 36323.90869140625\n",
      "epoch: 279/400, loss: 31609.69744873047\n",
      "epoch: 280/400, loss: 31191.515197753906\n",
      "epoch: 281/400, loss: 30734.11505126953\n",
      "epoch: 282/400, loss: 32853.9638671875\n",
      "epoch: 283/400, loss: 33468.85009765625\n",
      "epoch: 284/400, loss: 31194.526668548584\n",
      "epoch: 285/400, loss: 38270.191162109375\n",
      "epoch: 286/400, loss: 45992.366943359375\n",
      "epoch: 287/400, loss: 31825.48383331299\n",
      "epoch: 288/400, loss: 31921.53302001953\n",
      "epoch: 289/400, loss: 30953.060314178467\n",
      "epoch: 290/400, loss: 35418.359619140625\n",
      "epoch: 291/400, loss: 32960.70654296875\n",
      "epoch: 292/400, loss: 30865.280937194824\n",
      "epoch: 293/400, loss: 43530.27392578125\n",
      "epoch: 294/400, loss: 33960.922607421875\n",
      "epoch: 295/400, loss: 32961.4990234375\n",
      "epoch: 296/400, loss: 34618.024169921875\n",
      "epoch: 297/400, loss: 40576.59899902344\n",
      "epoch: 298/400, loss: 31149.534106254578\n",
      "epoch: 299/400, loss: 33261.52294921875\n",
      "epoch: 300/400, loss: 36346.434814453125\n",
      "epoch: 301/400, loss: 31371.152282714844\n",
      "epoch: 302/400, loss: 40563.66564941406\n",
      "epoch: 303/400, loss: 35273.756591796875\n",
      "epoch: 304/400, loss: 43325.219482421875\n",
      "epoch: 305/400, loss: 37859.918701171875\n",
      "epoch: 306/400, loss: 33204.1796875\n",
      "epoch: 307/400, loss: 30573.46954345703\n",
      "epoch: 308/400, loss: 33908.743896484375\n",
      "epoch: 309/400, loss: 30837.269927978516\n",
      "epoch: 310/400, loss: 31607.861389160156\n",
      "epoch: 311/400, loss: 37952.09997558594\n",
      "epoch: 312/400, loss: 31996.424102783203\n",
      "epoch: 313/400, loss: 48489.147216796875\n",
      "epoch: 314/400, loss: 32738.62368774414\n",
      "epoch: 315/400, loss: 35598.4912109375\n",
      "epoch: 316/400, loss: 31214.006797790527\n",
      "epoch: 317/400, loss: 31093.311798095703\n",
      "epoch: 318/400, loss: 30868.816680908203\n",
      "epoch: 319/400, loss: 30737.525390625\n",
      "epoch: 320/400, loss: 30991.287048339844\n",
      "epoch: 321/400, loss: 31594.510375976562\n",
      "epoch: 322/400, loss: 33515.82238769531\n",
      "epoch: 323/400, loss: 31808.047241210938\n",
      "epoch: 324/400, loss: 31057.291564941406\n",
      "epoch: 325/400, loss: 33773.10510253906\n",
      "epoch: 326/400, loss: 30524.356971740723\n",
      "epoch: 327/400, loss: 34318.90759277344\n",
      "epoch: 328/400, loss: 31791.903198242188\n",
      "epoch: 329/400, loss: 30628.940368652344\n",
      "epoch: 330/400, loss: 30590.31633758545\n",
      "epoch: 331/400, loss: 32276.325439453125\n",
      "epoch: 332/400, loss: 42367.57067871094\n",
      "epoch: 333/400, loss: 31250.60542678833\n",
      "epoch: 334/400, loss: 42179.46984863281\n",
      "epoch: 335/400, loss: 30890.759765625\n",
      "epoch: 336/400, loss: 42232.697021484375\n",
      "epoch: 337/400, loss: 31809.853912353516\n",
      "epoch: 338/400, loss: 33582.922607421875\n",
      "epoch: 339/400, loss: 41388.50354003906\n",
      "epoch: 340/400, loss: 32649.787719726562\n",
      "epoch: 341/400, loss: 30904.345056533813\n",
      "epoch: 342/400, loss: 34566.296142578125\n",
      "epoch: 343/400, loss: 30726.28257751465\n",
      "epoch: 344/400, loss: 30679.652923583984\n",
      "epoch: 345/400, loss: 30627.387145996094\n",
      "epoch: 346/400, loss: 30708.31039428711\n",
      "epoch: 347/400, loss: 31649.095581054688\n",
      "epoch: 348/400, loss: 30498.288681030273\n",
      "epoch: 349/400, loss: 49337.48583984375\n",
      "epoch: 350/400, loss: 43193.068359375\n",
      "epoch: 351/400, loss: 31794.950317382812\n",
      "epoch: 352/400, loss: 37664.97839355469\n",
      "epoch: 353/400, loss: 34606.733154296875\n",
      "epoch: 354/400, loss: 32931.66955566406\n",
      "epoch: 355/400, loss: 30535.904426574707\n",
      "epoch: 356/400, loss: 30414.231053709984\n",
      "epoch: 357/400, loss: 30415.096908569336\n",
      "epoch: 358/400, loss: 34106.822998046875\n",
      "epoch: 359/400, loss: 33226.80078125\n",
      "epoch: 360/400, loss: 34230.419189453125\n",
      "epoch: 361/400, loss: 57708.84423828125\n",
      "epoch: 362/400, loss: 34985.48229980469\n",
      "epoch: 363/400, loss: 35105.99853515625\n",
      "epoch: 364/400, loss: 32275.747802734375\n",
      "epoch: 365/400, loss: 31617.9501953125\n",
      "epoch: 366/400, loss: 37909.6552734375\n",
      "epoch: 367/400, loss: 34422.056396484375\n",
      "epoch: 368/400, loss: 30609.396508216858\n",
      "epoch: 369/400, loss: 30794.409545898438\n",
      "epoch: 370/400, loss: 30383.215267181396\n",
      "epoch: 371/400, loss: 32706.4833984375\n",
      "epoch: 372/400, loss: 30594.176483154297\n",
      "epoch: 373/400, loss: 31762.110595703125\n",
      "epoch: 374/400, loss: 34966.285888671875\n",
      "epoch: 375/400, loss: 30574.918975830078\n",
      "epoch: 376/400, loss: 30883.1416015625\n",
      "epoch: 377/400, loss: 35973.39025878906\n",
      "epoch: 378/400, loss: 31731.798583984375\n",
      "epoch: 379/400, loss: 30107.019371032715\n",
      "epoch: 380/400, loss: 30398.835243225098\n",
      "epoch: 381/400, loss: 31048.289916992188\n",
      "epoch: 382/400, loss: 37386.671142578125\n",
      "epoch: 383/400, loss: 30628.20086669922\n",
      "epoch: 384/400, loss: 30229.44757080078\n",
      "epoch: 385/400, loss: 30232.916816711426\n",
      "epoch: 386/400, loss: 31981.87255859375\n",
      "epoch: 387/400, loss: 30872.35333251953\n",
      "epoch: 388/400, loss: 33564.74658203125\n",
      "epoch: 389/400, loss: 37018.459716796875\n",
      "epoch: 390/400, loss: 32478.263793945312\n",
      "epoch: 391/400, loss: 53319.18786621094\n",
      "epoch: 392/400, loss: 33444.95202636719\n",
      "epoch: 393/400, loss: 30553.790420532227\n",
      "epoch: 394/400, loss: 32130.546630859375\n",
      "epoch: 395/400, loss: 30219.523010253906\n",
      "epoch: 396/400, loss: 41261.59619140625\n",
      "epoch: 397/400, loss: 31520.53158569336\n",
      "epoch: 398/400, loss: 35268.923583984375\n",
      "epoch: 399/400, loss: 30986.913848876953\n",
      "epoch: 400/400, loss: 30654.68975830078\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 400\n",
    "lr = 1e-3\n",
    "\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, val_loss = 0, 0\n",
    "    for data in diabet_train_dataloader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        output = model(data[0])\n",
    "        loss = loss_fn(output, data[1].unsqueeze(1))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "    print(f'epoch: {epoch + 1}/{EPOCHS}, loss: {train_loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговые результаты точности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 41.5276985168457\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in diabet_test_dataloader:\n",
    "        y_pred = model(inputs)\n",
    "        for prediction in y_pred:\n",
    "            prediction = prediction.numpy()\n",
    "            y_pred_list.append(prediction)\n",
    "        for label in labels:\n",
    "            label = label.numpy()\n",
    "            y_true.append(label)\n",
    "\n",
    "print(f'Mean Absolute Error : {mean_absolute_error(y_true, y_pred_list)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
